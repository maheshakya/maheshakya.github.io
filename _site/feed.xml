<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Espy Yonder</title>
		<description>This is my personal blog created in order to espy yonder</description>		
		<link>http://maheshakya.github.io/</link>
		<atom:link href="http://maheshakya.github.io//feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Performance evaluation of Approximate Nearest Neighbor search implementations - Part 1</title>
				<description>&lt;p&gt;This typifies the official instigation of my GSoC project. In my &lt;a href=&quot;/gsoc/2014/05/18/preparing-a-bench-marking-data-set-using-singula-value-decomposition-on-movielens-data.html&quot;&gt;previous post&lt;/a&gt;, I have discussed how to create the bench marking data set which will be used from here on. 
I will discuss how the evaluation framework is designed to evaluate the performance of the existing approximate nearest neighbor search implementations. For this evaluation, I have chosen the following implementations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spotify &lt;a href=&quot;https://github.com/spotify/annoy&quot;&gt;ANNOY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.ubc.ca/research/flann/&quot;&gt;FLANN&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.kgraph.org/&quot;&gt;KGraph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nearpy.io/&quot;&gt;nearpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://pypi.python.org/pypi/lshash/0.0.4dev&quot;&gt;lshash&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Evaluation framework&lt;/h2&gt;

&lt;p&gt;There are three main considerations in this evaluation framework. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Memory consumption&lt;/li&gt;
&lt;li&gt;Precision&lt;/li&gt;
&lt;li&gt;Query speed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For each of these aspects, there are separated tests which I will explain in the upcoming sections. In addition to these, from &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt; community, another requirement emerged to consider the index building time in order to assist with incremental learning. This is an optimization which has to be done in the ANN version that will be implemented in &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Note: The evaluation framework will be run on a system with following configurations:
* Memory : 16 GB (1600 MHz)
* CPU    : Intel® Core™ i7-4700MQ CPU @ 2.40GHz × 8 &lt;/p&gt;

&lt;p&gt;In this post, I will discuss the evaluation done for memory consumption.&lt;/p&gt;

&lt;h3&gt;Memory consumption of existing ANN methods&lt;/h3&gt;

&lt;p&gt;Memory consumption corresponds to the index building step in a nearest neighbor search data structure since it is the process which stores the data in the data structure accordingly. In this framework, there are two main aspects taken into account to express the memory consumption of an index building process. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Peak memory consumption&lt;/strong&gt;: While the index building process takes place, there is a maximum amount of memory used. No amount of memory beyond this peak memory will be consumed during this process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overall increment&lt;/strong&gt;: This is the actual memory used by the data structure after the index building process. This may be less than or equal to the peak memory consumption.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These two aspects of the above mentioned ANN implementations were measured. The results are as follows.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.google.com/drawings/d/1gCLDnk_UJ-kk5bkY-g-jp1hnqO5GcDqusQch2OHw0VI/pub?w=668&amp;amp;h=468&quot; alt=&quot;Memory_usage_table&quot;&gt;&lt;/p&gt;

&lt;p&gt;Following graph illustrates the peak memory consumptions in log scale.
&lt;img src=&quot;https://docs.google.com/drawings/d/1j7BjozhffmhVMbJHtymDYtlKFvxo1LQRQnHY2PHDIQU/pub?w=804&amp;amp;h=614&quot; alt=&quot;Peak_memory_usage&quot;&gt;&lt;/p&gt;

&lt;p&gt;Following graph illustrates the overall memory increments in log scale.
&lt;img src=&quot;https://docs.google.com/drawings/d/1EhBe1c45BIn5tEs6hzqF8MNMzrYs_NZEivii8Wqs0A0/pub?w=808&amp;amp;h=614&quot; alt=&quot;Overall_memory_increment&quot;&gt;&lt;/p&gt;

&lt;p&gt;In the upcoming posts, I will discuss the other two aspects in the evaluation framework in detail and the performance of LSH-Forest implementation as well. &lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</description>
				<pubDate>Sun, 25 May 2014 00:00:00 +0530</pubDate>
				<link>http://maheshakya.github.io//gsoc/2014/05/25/performance-evaluation-of-approximate-nearest-neighbor-search-implementations---part-1.html</link>
				<guid isPermaLink="true">http://maheshakya.github.io//gsoc/2014/05/25/performance-evaluation-of-approximate-nearest-neighbor-search-implementations---part-1.html</guid>
			</item>
		
			<item>
				<title>Singular value decomposition to create a bench marking data set from MovieLens data</title>
				<description>&lt;p&gt;This is the second article on my Google Summer of Code project and this follows from my &lt;a href=&quot;/gsoc/2014/05/04/approximate-nearest-neighbor-search-using-lsh.html&quot;&gt;previous post&lt;/a&gt; about the description about my project: Approximate nearest neighbor search using Locality sensitive hashing. Here, I will elaborate how I created my data set for prototyping, evaluating and bench marking purposes in the project. I have used &lt;a href=&quot;http://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;Singular value decomposition&lt;/a&gt; on the &lt;a href=&quot;http://grouplens.org/datasets/movielens/&quot;&gt;MovieLens 1M&lt;/a&gt; data to create this sample data set.&lt;/p&gt;

&lt;h2&gt;MovieLens 1M data&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://grouplens.org/&quot;&gt;GroupLens Research&lt;/a&gt; is and organization publishes research articles in conferences and journals primarily in the field of computer science, but also in other fields including psychology, sociology, and medicine. It has collected and made available rating data sets from the &lt;a href=&quot;http://movielens.org&quot;&gt;MovieLens&lt;/a&gt; web site. The data sets were collected over various periods of time, depending on the size of the set.&lt;/p&gt;

&lt;p&gt;MovieLens 1M data set contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000. After extracting the compressed content, there will be following files at your hand:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ratings.dat : Contains user IDs, movie IDs, ratings on 5 star scale and time stamp.&lt;/li&gt;
&lt;li&gt;movies.dat  : Contains movie IDs, titles and genres.&lt;/li&gt;
&lt;li&gt;users.dat   : Contains user IDs, genders, ages, ocupations and zip-codes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More information about this can be found in the &lt;a href=&quot;http://files.grouplens.org/datasets/movielens/ml-1m-README.txt&quot;&gt;README&lt;/a&gt;. &lt;/p&gt;

&lt;h2&gt;A brief explanation about singular value decomposition and its&amp;#39; role in machine learning&lt;/h2&gt;

&lt;p&gt;Singular value decomposition is a matrix factorization method. The general equation can be expressed as follows.&lt;/p&gt;

&lt;p&gt;$$X = USV^T$$&lt;/p&gt;

&lt;p&gt;Suppose \(X\) has \(n\) rows and \(d\) columns. \(U\) is a matrix whose dimensions are \(n \times n\), \(V\) is another matrix whose dimensions are \(d \times d\), and \(S\) is a matrix whose dimensions are \(n \times d\), the same dimensions as \(X\). 
In addition, \(U^T U = I _ n\) and \(V^T V = I _ d\)&lt;/p&gt;

&lt;p&gt;You can read and understand more about this decomposition method and how it work from this &lt;a href=&quot;http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition&quot;&gt;article&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;What is the significance of the SVD(Singular Value Decomposition) in machine learning and what does it have to do with MovieLens data?&lt;/h3&gt;

&lt;p&gt;We can represent each movie from a dimension and each user corresponds to a data point in this high dimensional space. But we are not able to visualize more than three dimensions. This data can be represented by a matrix(The \(X\) in the above equation). A sample depiction of the matrix may look as follows. 
&lt;img src=&quot;https://docs.google.com/drawings/d/1oBQ7iNf-c6GCYBvalyM7HXlcscX1ATz9lQsxzpHdCyQ/pub?w=960&amp;amp;h=720&quot; alt=&quot;user_movie_matrix&quot;&gt;
Because number of ratings for a movie by users is significantly low when considered with the number of users, this matrix contains a large number of empty entries. Therefore this matrix will be a very sparse matrix. Hence, approximating this matrix with a lower rank matrix is a worthwhile attempt.&lt;/p&gt;

&lt;p&gt;Consider the following scenario:&lt;/p&gt;

&lt;p&gt;If every user who likes &amp;quot;movie X&amp;quot; also likes &amp;quot;movie Y&amp;quot;, then it is possible to group them together to form an agglomerative movie or feature. After forming new features in that way, two users can be compared by analyzing their ratings for different features rather than for individual movies.&lt;/p&gt;

&lt;p&gt;In the same way different users may rate same movies similarly. So there can different types of similarities among user preferences.&lt;/p&gt;

&lt;p&gt;According to this factorization method (you might want to read more about SVD at this point from the reference I have provided earlier) the matrix \(S\) is a diagonal matrix containing the singular values of the matrix \(X\). The number of singular values is exactly equal to the rank of the matrix \(X\). The rank of a matrix is the number of linearly independent rows or columns in the matrix. We know that two vectors are linearly independent if they cannot be written as the sum or scalar multiple of any other vectors in that vector space. You can notice that this linear independence somehow captures the notion of a feature or agglomerative item which we try to generate from in this approach. According to the above scenario, if every user who liked &amp;quot;Movie X&amp;quot; also liked &amp;quot;Movie Y&amp;quot;, then those two movie vectors would be linearly dependent and would only contribute one to the rank.&lt;/p&gt;

&lt;p&gt;So how are we to get rid of this redundant data. We can compare movies if most users who like one also like the other. In order to do that, we will keep the largest k singular values in \(S\). This will give us the best rank-k approximation to X. &lt;/p&gt;

&lt;p&gt;So the entire procedure can be boiled down to following three steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Compute the SVD: \(X = U S V^T\).&lt;/li&gt;
&lt;li&gt;Form the matrix \(S&amp;#39;\) by keeping the k largest singular values and setting the others to zero.&lt;/li&gt;
&lt;li&gt;Form the matrix \(X _ lr\) by \(X _ lr = U S&amp;#39; V^T\).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Implementation&lt;/h2&gt;

&lt;p&gt;To perform SVD on MovieLens data set and recompose the matrix with a lower rank, I used scipy sparse matrix, numpy and pandas. It has been done in following steps.&lt;/p&gt;

&lt;p&gt;1)  Import required packages.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.sparse.linalg&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;svds&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pickle&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;2)  Load data set into a pandas data frame.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;data_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r&amp;#39;ratings.dat&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;::&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, have assumed that the &lt;code&gt;ratings.dat&lt;/code&gt; file from MovieLens 1M data will be in the working directory. Only reason I am using pandas data frame is its&amp;#39; convenience of usage. You can directly open the file and proceed. But then you will have to change following steps to adapt to method.&lt;/p&gt;

&lt;p&gt;3)  Extract required meta information from the data set.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
 
&lt;span class=&quot;n&quot;&gt;number_of_rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;number_of_columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;movie_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;movie_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movies&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;user_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As the user IDs and movie IDs are not consecutive number, a proper mapping is required. It will be used when inserting data into the matrix. At this point, you can delete the loaded data frame in order to save memory. But it is optional.&lt;/p&gt;

&lt;p&gt;4)  Creating the sparse matrix and inserting data.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#scipy sparse matrix to store the 1M matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lil_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;number_of_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#adds data into the sparse matrix&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gona&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movie_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can save sparse matrix &lt;code&gt;V&lt;/code&gt; using &lt;code&gt;pickle&lt;/code&gt; if you are willing to use it later. &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#as these operations consume a lot of time, it&amp;#39;s better to save processed data &lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;movielens_1M.pickle&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;5)  Perform SVD.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#as these operations consume a lot of time, it&amp;#39;s better to save processed data &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#gets SVD components from 10M matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;svds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;movielens_1M_svd_u.pickle&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;movielens_1M_svd_s.pickle&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;movielens_1M_svd_vt.pickle&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;svds&lt;/code&gt; method performs the SVD. Parameter &lt;code&gt;k&lt;/code&gt; is the number of singular values we want to retain. Here also I have save the intermediate data using &lt;code&gt;pickle&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After this decomposition you will get &lt;code&gt;u&lt;/code&gt;, &lt;code&gt;s&lt;/code&gt; and &lt;code&gt;vt&lt;/code&gt;. They have (&lt;code&gt;number of users&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt;), (&lt;code&gt;k&lt;/code&gt;, ) and (&lt;code&gt;k&lt;/code&gt;, &lt;code&gt;number of movies&lt;/code&gt;) shapes respectively.&lt;/p&gt;

&lt;p&gt;6)  Recomposing the lower rank matrix.&lt;/p&gt;

&lt;p&gt;As &lt;code&gt;s&lt;/code&gt; is a vector, we need to create a diagonal matrix form that with the diagonal containing the values of that vector. It is done as follows:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;s_diag_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s_diag_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will create a diagonal matrix. After that, all you have to do is get the matrix product of &lt;code&gt;u&lt;/code&gt;, &lt;code&gt;s_diag_matix&lt;/code&gt; and &lt;code&gt;vt&lt;/code&gt; in that order.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;X_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_diag_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have the lower rank approximation for \(X\) as \(X _ lr = U S&amp;#39; V^T\). Now this matrix can be used as a bench marking data set for the application.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Dimensionality Reduction and the Singular Value Decomposition, Available[online]: &lt;a href=&quot;http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/svd.html&quot;&gt;http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/svd.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data Mining Algorithms In R/Dimensionality Reduction/Singular Value Decomposition, Available[online]: &lt;a href=&quot;http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition&quot;&gt;http://en.wikibooks.org/wiki/Data&lt;em&gt;Mining&lt;/em&gt;Algorithms&lt;em&gt;In&lt;/em&gt;R/Dimensionality&lt;em&gt;Reduction/Singular&lt;/em&gt;Value_Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Sun, 18 May 2014 00:00:00 +0530</pubDate>
				<link>http://maheshakya.github.io//gsoc/2014/05/18/preparing-a-bench-marking-data-set-using-singula-value-decomposition-on-movielens-data.html</link>
				<guid isPermaLink="true">http://maheshakya.github.io//gsoc/2014/05/18/preparing-a-bench-marking-data-set-using-singula-value-decomposition-on-movielens-data.html</guid>
			</item>
		
			<item>
				<title>GSoC2014: Approximate nearest neighbor search using LSH</title>
				<description>&lt;p&gt;This project has been initiated as a Google summer of code project. &lt;a href=&quot;https://www.python.org/&quot;&gt;Python Software foundation&lt;/a&gt; serves as an &amp;quot;umbrella organization&amp;quot; to a variety of Python related open source projects. &lt;a href=&quot;http://scikit-learn.org/stable/index.html&quot;&gt;Scikit-learn&lt;/a&gt; is a machine learning module which operates under that umbrella. I&amp;#39;m instigating this project under that module. In the following sections, I will describe what this really is with help of the essence of my project proposal. This is just a head start of the journey and you will be able to obtain a clear picture as this proceeds.&lt;/p&gt;

&lt;h2&gt;The struggle for nearest neighbor search&lt;/h2&gt;

&lt;p&gt;Nearest neighbor search is a well known problem which can be defined as follows: given a collection of n data points, create a data structure which, given any query point, reports the data points that is closest to the query. This problem holds a major importance in certain applications: data mining, databases, data analysis, pattern recognition, similarity search, machine learning, image and video processing, information retrieval and statistics. To perform nearest neighbor search, there are several efficient algorithms known for the case where the dimension is low. But those methods suffer from either space or query time that is exponential in dimension.&lt;/p&gt;

&lt;p&gt;In order to address the “Curse of Dimensionality” in large data sets, recent researches had been lead based on approximating neighbor search. It has been proven that in many cases, approximate nearest neighbor is as almost good as the exact one[1]. Locality Sensitive Hashing is one of those approximating methods. The key idea of LSH is to hash data points using several hash functions to ensure that for each function the probability of collision is much higher for objects that are close to each other than for those that are far apart.&lt;/p&gt;

&lt;h2&gt;What does this have to do with me?&lt;/h2&gt;

&lt;p&gt;In scikit-learn, currently exact nearest neighbor search is implemented, but when it comes to higher dimensions, it fails to perform efficiently[2]. So I&amp;#39;m taking an initiative to implement LSH based ANN for scikit-learn. In this project, several variants of LSH-ANN methods will be prototyped and evaluated. After identifying the most appropriate method for scikit-learn, it will be implemented in accordance with scikit-learn&amp;#39;s API and documentation levels, which includes narrative documentation. Then with the results obtained from prototyping stage, storing and querying structure of ANN will be implemented. After that, ANN part will be integrated into &lt;code&gt;sklearn.neighbors&lt;/code&gt; module. Next comes the application of this method. Most of clustering algorithms use nearest neighbor search, therefore this method will be adapted to use in those modules in order to improve their operational speed. As these activities proceed, testing, examples and documentation will be covered. Bench marking will be done to assess the implementation.&lt;/p&gt;

&lt;h2&gt;Milestones of the project&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Prototyping/Evaluating existing LSH based ANN methods (vanilla and others) in order to find the most appropriate method to have in scikit-learn. There is no point of having methods in scikit-learn which are impractical to use with real data.&lt;/li&gt;
&lt;li&gt;Approximating neighbor search uses hashing algorithms of LSH family. These algorithms will be implemented.&lt;/li&gt;
&lt;li&gt;Implementation of an efficient storing structure to retain trained/hashed data.&lt;/li&gt;
&lt;li&gt;Integrating the ANN search into current implementation of neighbor search, so that this can be used with the existing API.&lt;/li&gt;
&lt;li&gt;Improving speed of existing clustering models with the implemented ANN search.&lt;/li&gt;
&lt;li&gt;Completing tests, examples, documentation and bench marking.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Well that&amp;#39;s it for the moment. I will guide you through when the things are in motion. &lt;/p&gt;

&lt;h2&gt;Abbreviations&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;LSH : Locality sensitive hashing&lt;/li&gt;
&lt;li&gt;ANN : Approximate nearest neighbor&lt;/li&gt;
&lt;li&gt;API : Application programming interface&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;A. Andoni and P. Indyk,&amp;quot;Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions&amp;quot;, Available[online]:&lt;a href=&quot;http://people.csail.mit.edu/indyk/p117-andoni.pdf&quot;&gt;http://people.csail.mit.edu/indyk/p117-andoni.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;R. Rehurek, &amp;quot;Performance Shootout of Nearest Neighbours: Contestants&amp;quot;, Available[online]:&lt;a href=&quot;http://radimrehurek.com/2013/12/performance-shootout-of-nearest-neighbours-contestants/&quot;&gt;http://radimrehurek.com/2013/12/performance-shootout-of-nearest-neighbours-contestants/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Sun, 04 May 2014 15:44:56 +0530</pubDate>
				<link>http://maheshakya.github.io//gsoc/2014/05/04/approximate-nearest-neighbor-search-using-lsh.html</link>
				<guid isPermaLink="true">http://maheshakya.github.io//gsoc/2014/05/04/approximate-nearest-neighbor-search-using-lsh.html</guid>
			</item>
		
			<item>
				<title>The Pythonizer!</title>
				<description>&lt;p&gt;Though I&amp;#39;m a Python fanatic, Jekyll is inconceivably awesome! (It&amp;#39;s Ruby)&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Hi, &amp;quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;Tom&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#=&amp;gt; prints &amp;#39;Hi, Tom&amp;#39; to STDOUT.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/mojombo/jekyll&quot;&gt;Jekyll&amp;#39;s GitHub repo&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 06 Apr 2014 21:10:56 +0530</pubDate>
				<link>http://maheshakya.github.io//jekyll/update/2014/04/06/welcome-to-jekyll.html</link>
				<guid isPermaLink="true">http://maheshakya.github.io//jekyll/update/2014/04/06/welcome-to-jekyll.html</guid>
			</item>
		
	</channel>
</rss>
